{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyID3 and MyBaggingID3 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for\n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           positive (`1`) or negative (`0`)\n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    p1 = np.sum(y) / len(y)\n",
    "    if p1 == 0 or p1 == 1:\n",
    "        return 0\n",
    "    h = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "    return h\n",
    "\n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):  List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "\n",
    "    Returns:\n",
    "        left_indices (list): Indices with feature value == 1\n",
    "        right_indices (list): Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    ## loop over the list node_indices (the train set)\n",
    "    for index in node_indices:\n",
    "        if X[index][feature] == 0:\n",
    "            right_indices.append(index)\n",
    "        else:\n",
    "            left_indices.append(index)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "\n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    Returns:\n",
    "        cost (float):        Cost computed\"\"\"\n",
    "    # create left and right branches\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    w_l = len(left_indices) / len(node_indices)\n",
    "    w_r = len(right_indices) / len(node_indices)\n",
    "    # calculate the y in the training data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for index in node_indices:\n",
    "        X_train.append(X[index])\n",
    "        y_train.append(y[index])\n",
    "    # calculate the y of the left and right branches\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "    for index in left_indices:\n",
    "        y_left.append(y[index])\n",
    "    for index in right_indices:\n",
    "        y_right.append(y[index])\n",
    "    ig = compute_entropy(y_train) - (w_l * compute_entropy(y_left) + w_r * compute_entropy(y_right))\n",
    "    return ig\n",
    "\n",
    "\n",
    "def get_best_split(X, y, node_indices):\n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"\n",
    "    feature_list = np.arange(len(X[0])).tolist()\n",
    "    best_result = 0\n",
    "    best_feature = \"nan\"\n",
    "    # loop ove feature list in order to see whch feature have the best split\n",
    "    for feature in feature_list:\n",
    "        ig = compute_information_gain(X, y, node_indices, feature)\n",
    "        if ig > best_result:\n",
    "            best_result = ig\n",
    "            best_feature = feature\n",
    "    return best_feature\n",
    "\n",
    "\n",
    "def predict(X, y, node_indices):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X: matrix\n",
    "    :param y: output of matrix\n",
    "    :param node_indices: the indices which exsits in the node\n",
    "    :return:\n",
    "     :param prediciton: what is th output of the node\n",
    "    \"\"\"\n",
    "    # define count varaible which count the number of the 1/0\n",
    "    zero = 0\n",
    "    one = 0\n",
    "    # loop over the indices in the node\n",
    "    for i in node_indices:\n",
    "        if y[i] == 0:\n",
    "            zero += 1\n",
    "        else:\n",
    "            one += 1\n",
    "    # decide who is biggger (one or zero)\n",
    "    result_pred = 0\n",
    "    if one > zero:\n",
    "        result_pred = 1, one / (zero + one)\n",
    "    prob_0 = zero / (zero + one)\n",
    "    prob_1 = one / (zero + one)\n",
    "    return result_pred, prob_0, prob_1\n",
    "\n",
    "\n",
    "def max_sample_n_features(X, y, max_samples, max_features):\n",
    "    # define the percentage of rows and columns to keep\n",
    "    row_percent_to_keep = max_samples\n",
    "    col_percent_to_keep = max_features\n",
    "\n",
    "    # compute the number of rows and columns to keep\n",
    "    n_rows_to_keep = int(X.shape[0] * row_percent_to_keep)\n",
    "    n_cols_to_keep = int(X.shape[1] * col_percent_to_keep)\n",
    "    # generate random row and column indices for the rows and columns to keep\n",
    "    row_indices = np.random.choice(X.shape[0], size=n_rows_to_keep, replace=True)\n",
    "    col_indices = np.random.choice(X.shape[1], size=n_cols_to_keep, replace=False)\n",
    "    # take the selected rows and columns from X_train and y_train\n",
    "    selected_X_train = X[row_indices][:, col_indices]\n",
    "    selected_y_train = y[row_indices]\n",
    "    return selected_X_train, selected_y_train, col_indices\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, X, y, node_indices, terminal=False, depth=0, right=None, left=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (array): matrix 2d array\n",
    "            y (array): aray\n",
    "            node_indices (array): array of the relevenat rows\n",
    "            terminal (bool, optional): flag if it  final node. Defaults to False.\n",
    "            depth (int, optional): the depth of the node. Defaults to 0.\n",
    "            right (node, optional): root. Defaults to None.\n",
    "            left (node, optional): root. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.terminal = terminal\n",
    "        self.depth = depth\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.node_indices = node_indices\n",
    "        self.feature = get_best_split(X, y, node_indices)\n",
    "        self.predict, self.predict_probab_0, self.predict_probab_1 = predict(X, y, node_indices)\n",
    "        if self.feature != \"nan\":\n",
    "            self.split()\n",
    "        else:\n",
    "            self.terminal = True\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        split the trre into 2 new nodes (if there's option to split)\n",
    "        \"\"\"\n",
    "        self.left, self.right = split_dataset(self.X, self.node_indices, self.feature)\n",
    "        if len(self.left) == 0 or len(self.right) == 0:\n",
    "            self.terminal = True\n",
    "        self.left = Node(self.X, self.y, self.left, self.terminal, depth=(self.depth + 1))\n",
    "        self.right = Node(self.X, self.y, self.right, self.terminal, depth=(self.depth + 1))\n",
    "\n",
    "\n",
    "class MyID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initial function of the class MYID3\n",
    "        Args:\n",
    "            max_depth (int, optional): the maximum depth of the required\n",
    "                tree. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.node_tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        \"\"\"\n",
    "        node_indices = np.arange(0, len(X), 1)\n",
    "        self.node_tree = Node(X, y, node_indices)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        Returns:\n",
    "            Prob (ndarray)                of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        y_prediced = []\n",
    "        # loop over the X matrix, and check each sample\n",
    "        for x in X:\n",
    "            temp_node = self.node_tree\n",
    "            while self.max_depth != temp_node.depth and temp_node.terminal == False:\n",
    "                if x[temp_node.feature] == 1:\n",
    "                    temp_node = temp_node.left\n",
    "                else:\n",
    "                    temp_node = temp_node.right\n",
    "            if type(temp_node.predict) is not int:\n",
    "                y_prediced.append(round(temp_node.predict[0]))\n",
    "            else:\n",
    "                y_prediced.append(round(temp_node.predict))\n",
    "        return np.array(y_prediced)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        Returns:\n",
    "            Prob (ndarray)           of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        y_prediced_prob = []\n",
    "        for x in X:\n",
    "            predict_0_1 = []\n",
    "            temp_node = self.node_tree\n",
    "            while self.max_depth != temp_node.depth and temp_node.terminal == False:\n",
    "                if x[temp_node.feature] == 1:\n",
    "                    temp_node = temp_node.left\n",
    "                else:\n",
    "                    temp_node = temp_node.right\n",
    "            predict_0_1.append(round(temp_node.predict_probab_0))\n",
    "            predict_0_1.append(round(temp_node.predict_probab_1))\n",
    "            y_prediced_prob.append(predict_0_1)\n",
    "        return np.array(y_prediced_prob)\n",
    "\n",
    "\n",
    "class MyBaggingID3:\n",
    "    def __init__(self, max_depth=None, n_estimators=2, max_samples=1, max_features=1):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            max_depth (int, optional): _description_. Defaults to None.\n",
    "            n_estimators (int, optional): The number of ID3 in the ensemble_. Defaults to 2.\n",
    "            max_samples (int, optional):  A float num that represents the fraction of samples to draw from X  with replacement  to train each base estimator. Defaults to 1.\n",
    "            max_features (int , optional): A float num (between 0 to 1) that represents the fraction of features to draw from X without replacement. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.forest = []\n",
    "        self.features = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        \"\"\"\n",
    "        node_indices = np.arange(0, len(X), 1)\n",
    "        # create new x train any y train according to the hyperparameters\n",
    "        # create forest ( list of trees)\n",
    "        for tree in range(self.n_estimators):\n",
    "            X_s, y_s, col_indices = max_sample_n_features(X, y, self.max_samples, self.max_features)\n",
    "            tree = MyID3(max_depth=self.max_depth)\n",
    "            tree.fit(X_s, y_s)\n",
    "            self.forest.append(tree)\n",
    "            self.features.append(col_indices)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        Returns:\n",
    "            Prob (ndarray)                of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # loop over each sample in X matrix\n",
    "        y_prediced = []\n",
    "        for x in X:\n",
    "            tree_preidct = []\n",
    "            i = 0\n",
    "            # loop over each tree in the forest list\n",
    "            for tree in self.forest:\n",
    "                col = self.features[i]\n",
    "                i += 1\n",
    "                temp_node = tree.node_tree\n",
    "                x_res = x[col]\n",
    "                while self.max_depth != temp_node.depth and temp_node.terminal == False:\n",
    "                    if x_res[temp_node.feature] == 1:\n",
    "                        temp_node = temp_node.left\n",
    "                    else:\n",
    "                        temp_node = temp_node.right\n",
    "                # insert the result of each tree to list\n",
    "                tree_preidct.append(temp_node.predict)\n",
    "\n",
    "            # each  the final result to the list, the final result by majority voting\n",
    "            pred = max(set(tree_preidct), key=tree_preidct.count)\n",
    "            if type(pred) is not int:\n",
    "                y_prediced.append(pred[0])\n",
    "            else:\n",
    "                y_prediced.append(pred)\n",
    "        return np.array(y_prediced)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        Returns:\n",
    "            Prob (ndarray)           of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # loop over each sample in the matrix\n",
    "        y_prediced_prob = []\n",
    "        for x in X:\n",
    "            # loop over each tree in the forest list\n",
    "            tree_predict_prob = []\n",
    "            i = 0\n",
    "            for tree in self.forest:\n",
    "                predict_0_1 = []\n",
    "                col = self.features[i]\n",
    "                i += 1\n",
    "                temp_node = tree.node_tree\n",
    "                x_res = x[col]\n",
    "                while self.max_depth != temp_node.depth and temp_node.terminal == False:\n",
    "                    if x_res[temp_node.feature] == 1:\n",
    "                        temp_node = temp_node.left\n",
    "                    else:\n",
    "                        temp_node = temp_node.right\n",
    "                # inset each result of the tree to the tree_predict_prob\n",
    "                predict_0_1.append(temp_node.predict_probab_0)\n",
    "                predict_0_1.append(temp_node.predict_probab_1)\n",
    "                tree_predict_prob.append(predict_0_1)\n",
    "            # calclualte the avearge of the forest over each sample\n",
    "            y_prediced_prob.append(np.mean(np.array(tree_predict_prob), axis=0))\n",
    "        return np.array(y_prediced_prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ***Tic Tac Toe dataset***\n",
    "\"\"\"\n",
    "Number of Instances: 958\n",
    "Number of Attributes: 10\n",
    "Attribute values: x=player x has taken, o=player o has taken, b=blank\n",
    "Class Distribution: About 65.3% are positive (i.e., wins for \"x\")\n",
    "No missing values\n",
    "\"\"\"\n",
    "DF1 = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\",\n",
    "    header=None,\n",
    ")\n",
    "DF1.columns = [\n",
    "    \"top-left-square\",\n",
    "    \"top-middle-square\",\n",
    "    \"top-right-square\",\n",
    "    \"middle-left-square\",\n",
    "    \"middle-middle-square\",\n",
    "    \"middle-right-square\",\n",
    "    \"bottom-left-square\",\n",
    "    \"bottom-middle-square\",\n",
    "    \"bottom-right-square\",\n",
    "    \"Class\",\n",
    "]\n",
    "\n",
    "\n",
    "# ***Lymphography dataset***\n",
    "\"\"\"\n",
    "Number of Instances: 148\n",
    "Number of Attributes: 19\n",
    "Attribute values: \n",
    "    1. class: normal find, metastases, malign lymph, fibrosis\n",
    "    2. lymphatics: normal, arched, deformed, displaced\n",
    "    3. block of affere: no, yes\n",
    "    4. bl. of lymph. c: no, yes\n",
    "    5. bl. of lymph. s: no, yes\n",
    "    6. by pass: no, yes\n",
    "    7. extravasates: no, yes\n",
    "    8. regeneration of: no, yes\n",
    "    9. early uptake in: no, yes\n",
    "   10. lym.nodes dimin: 0-3\n",
    "   11. lym.nodes enlar: 1-4\n",
    "   12. changes in lym.: bean, oval, round\n",
    "   13. defect in node: no, lacunar, lac. marginal, lac. central\n",
    "   14. changes in node: no, lacunar, lac. margin, lac. central\n",
    "   15. changes in stru: no, grainy, drop-like, coarse, diluted, reticular, \n",
    "                        stripped, faint, \n",
    "   16. special forms: no, chalices, vesicles\n",
    "   17. dislocation of: no, yes\n",
    "   18. exclusion of no: no, yes\n",
    "   19. no. of nodes in: 0-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, >=70\n",
    "Class Distribution: 54.72% metastases, 41.21% malign lymph, 2.7% fibrosis, 1.3% normal find\n",
    "No missing values\n",
    "\"\"\"\n",
    "DF2 = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/lymphography/lymphography.data\",\n",
    "    header=None,\n",
    ")\n",
    "DF2.columns = [\n",
    "    \"Class\",\n",
    "    \"lymphatics\",\n",
    "    \"block of affere\",\n",
    "    \"bl. of lymph. c\",\n",
    "    \"bl. of lymph. s\",\n",
    "    \"by pass\",\n",
    "    \"extravasates\",\n",
    "    \"regeneration of\",\n",
    "    \"early uptake in\",\n",
    "    \"lym.nodes dimin\",\n",
    "    \"lym.nodes enlar\",\n",
    "    \"changes in lym.\",\n",
    "    \"defect in node\",\n",
    "    \"changes in node\",\n",
    "    \"changes in stru\",\n",
    "    \"special forms\",\n",
    "    \"dislocation of\",\n",
    "    \"exclusion of no\",\n",
    "    \"no. of nodes in\",\n",
    "]\n",
    "\n",
    "temp_ind = DF2[DF2[\"Class\"].isin([1, 4])].index\n",
    "DF2.drop(temp_ind, inplace=True)\n",
    "cols = DF2.columns.tolist()\n",
    "cols = cols[1:] + cols[0:1]\n",
    "DF2 = DF2[cols]\n",
    "\n",
    "\n",
    "# ***Chess kr-vs-kp dataset***\n",
    "\"\"\"\n",
    "Number of Instances: 3196\n",
    "Number of Attributes: 36\n",
    "Attribute values: The format for instances in this database is a sequence of 37 attribute values.\n",
    "Each instance is a board-descriptions for this chess endgame.  The first\n",
    "36 attributes describe the board.  The last (37th) attribute is the\n",
    "classification: \"win\" or \"nowin\".  There are 0 missing values.\n",
    "A typical board-description is \"f,f,f,f,f,f,f,f,f,f,f,f,l,f,n,f,f,t,f,f,f,f,f,f,f,t,f,f,f,f,f,f,f,t,t,n,won\"\n",
    "The names of the features do not appear in the board-descriptions.\n",
    "Instead, each feature correponds to a particular position in the\n",
    "feature-value list.  For example, the head of this list is the value\n",
    "for the feature \"bkblk\".  The following is the list of features, in\n",
    "the order in which their values appear in the feature-value list:\n",
    "[bkblk,bknwy,bkon8,bkona,bkspr,bkxbq,bkxcr,bkxwp,blxwp,bxqsq,cntxt,dsopp,dwipd,\n",
    " hdchk,katri,mulch,qxmsq,r2ar8,reskd,reskr,rimmx,rkxwp,rxmsq,simpl,skach,skewr,\n",
    " skrxp,spcop,stlmt,thrsk,wkcti,wkna8,wknck,wkovl,wkpos,wtoeg]\n",
    "No missing values\n",
    "\"\"\"\n",
    "DF3 = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data\",\n",
    "    header=None,\n",
    ")\n",
    "DF3.columns = [\n",
    "    \"bkblk\",\n",
    "    \"bknwy\",\n",
    "    \"bkon8\",\n",
    "    \"bkona\",\n",
    "    \"bkspr\",\n",
    "    \"bkxbq\",\n",
    "    \"bkxcr\",\n",
    "    \"bkxwp\",\n",
    "    \"blxwp\",\n",
    "    \"bxqsq\",\n",
    "    \"cntxt\",\n",
    "    \"dsopp\",\n",
    "    \"dwipd\",\n",
    "    \"hdchk\",\n",
    "    \"katri\",\n",
    "    \"mulch\",\n",
    "    \"qxmsq\",\n",
    "    \"r2ar8\",\n",
    "    \"reskd\",\n",
    "    \"reskr\",\n",
    "    \"rimmx\",\n",
    "    \"rkxwp\",\n",
    "    \"rxmsq\",\n",
    "    \"simpl\",\n",
    "    \"skach\",\n",
    "    \"skewr\",\n",
    "    \"skrxp\",\n",
    "    \"spcop\",\n",
    "    \"stlmt\",\n",
    "    \"thrsk\",\n",
    "    \"wkcti\",\n",
    "    \"wkna8\",\n",
    "    \"wknck\",\n",
    "    \"wkovl\",\n",
    "    \"wkpos\",\n",
    "    \"wtoeg\",\n",
    "    \"Class\",\n",
    "]\n",
    "\n",
    "\n",
    "# ***Car Evaluation dataset***\n",
    "\"\"\"\n",
    "Number of Instances: 1728\n",
    "Number of Attributes: 7\n",
    "Attribute values: \n",
    "    1. buying: v-high, high, med, low\n",
    "    2. maint: v-high, high, med, low\n",
    "    3. doors: 2, 3, 4, 5-more\n",
    "    4. persons: 2, 4, more\n",
    "    5. lug_boot: small, med, big\n",
    "    6. safety: low, med, high\n",
    "Class Distribution: 70.023% are unacceptable, 22.222% are acceptable, 3.993% are good and 3.762% are very good\n",
    "No missing values\n",
    "\"\"\"\n",
    "DF4 = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "    header=None,\n",
    ")\n",
    "DF4.columns = [\n",
    "    \"buying\",\n",
    "    \"maint\",\n",
    "    \"doors\",\n",
    "    \"persons\",\n",
    "    \"lug_boot\",\n",
    "    \"safety\",\n",
    "    \"Class\",\n",
    "]\n",
    "# Makes the target classes binary - merging the acceptable, good and very good classifications\n",
    "DF4[\"Class\"] = DF4[\"Class\"].replace([\"good\", \"vgood\"], \"acc\")\n",
    "\n",
    "\n",
    "# ***Breast Cancer dataset***\n",
    "\"\"\"\n",
    "Number of Instances: 286\n",
    "Number of Attributes: 10\n",
    "Attribute values: \n",
    "    1. Class: no-recurrence-events, recurrence-events\n",
    "    2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "    3. menopause: lt40, ge40, premeno.\n",
    "    4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,\n",
    "                    45-49, 50-54, 55-59.\n",
    "    5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,\n",
    "                    27-29, 30-32, 33-35, 36-39.\n",
    "    6. node-caps: yes, no.\n",
    "    7. deg-malig: 1, 2, 3.\n",
    "    8. breast: left, right.\n",
    "    9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
    "    10. irradiat:\tyes, no.\n",
    "Class Distribution: 70.027% are no-reccurence-events, 29.72% are recurrence-events\n",
    "9 missing values in total, 8 from attribute #6 and 1 from attribute #9\n",
    "\"\"\"\n",
    "DF5 = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\",\n",
    "    header=None,\n",
    ")\n",
    "# Moves the target column from the first index to the last index\n",
    "cols = DF5.columns.tolist()\n",
    "cols = cols[1:] + cols[0:1]\n",
    "DF5 = DF5[cols]\n",
    "DF5.columns = [\n",
    "    \"age\",\n",
    "    \"menopause\",\n",
    "    \"tumor-size\",\n",
    "    \"inv-nodes\",\n",
    "    \"node-caps\",\n",
    "    \"deg-malig\",\n",
    "    \"breast\",\n",
    "    \"breast-quad\",\n",
    "    \"irradiat\",\n",
    "    \"Class\",\n",
    "]\n",
    "# Droping missing values\n",
    "temp_ind1 = DF5[DF5[\"node-caps\"] == \"?\"].index\n",
    "temp_ind2 = DF5[DF5[\"breast-quad\"] == \"?\"].index\n",
    "DF5.drop(temp_ind1, inplace=True)\n",
    "DF5.drop(temp_ind2, inplace=True)\n",
    "\n",
    "\n",
    "# The collection of 5 datasets\n",
    "DFS = {\n",
    "    \"Tic Tac Toe\": DF1,\n",
    "    \"Lymphography\": DF2,\n",
    "    \"Chess kr-vs-kp\": DF3,\n",
    "    \"Car Evaluation\": DF4,\n",
    "    \"Breast Cancer\": DF5,\n",
    "}\n",
    "\n",
    "\n",
    "# Defines the evaluation metrics\n",
    "METRICS = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"roc_auc_score\": roc_auc_score,\n",
    "}\n",
    "\n",
    "# The methods\n",
    "MODELS = [\"ID3\", \"MyID3\", \"BaggingID3\", \"MyBaggingID3\"]\n",
    "\n",
    "\n",
    "# Prepares the data for modeling\n",
    "def prepare_data(dataset):\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    lb = LabelBinarizer()\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    # One hot encoding the features\n",
    "    X_bin = ohe.fit_transform(X)\n",
    "    # The target feature is binarized\n",
    "    y_bin = lb.fit_transform(y).ravel()\n",
    "\n",
    "    return X_bin, y_bin\n",
    "\n",
    "\n",
    "# Evaluates the models\n",
    "def evaluate_models(X, y, metrics, df_name, table):\n",
    "    # Initializing the figure and subplots to log\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3, figsize=(16, 12))\n",
    "    axis = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "    ax_ind = 0\n",
    "\n",
    "    # Initializing W&B and KFold\n",
    "    wandb.init(project=\"ID3 & BaggingID3\", name=df_name, group=\"Report\", reinit=True)\n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=14)\n",
    "\n",
    "    run_times = {model_name: 0 for model_name in MODELS}\n",
    "    mean_results = {model_name: [] for model_name in MODELS}\n",
    "    results = {model_name: [] for model_name in MODELS}\n",
    "\n",
    "    # For each metric evaluate all models\n",
    "    for metric_name, metric_fn in metrics.items():\n",
    "        for model_name in MODELS:\n",
    "            start = time.time()\n",
    "            # For each split\n",
    "            for train_idx, test_idx in rkf.split(X):\n",
    "                models = {\n",
    "                    \"ID3\": DecisionTreeClassifier(max_depth=8, random_state=14),\n",
    "                    \"MyID3\": MyID3(max_depth=8),\n",
    "                    \"BaggingID3\": BaggingClassifier(\n",
    "                        DecisionTreeClassifier(max_depth=8, random_state=14),\n",
    "                        n_estimators=10,\n",
    "                        random_state=14,\n",
    "                        max_samples=0.8,\n",
    "                        max_features=0.8,\n",
    "                    ),\n",
    "                    \"MyBaggingID3\": MyBaggingID3(\n",
    "                        max_depth=8, n_estimators=10, max_samples=0.8, max_features=0.8\n",
    "                    ),\n",
    "                }\n",
    "                model = models[model_name]\n",
    "                # Spliting the data into train and test sets\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "                # Training the model on the training set\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluating the model on the test set\n",
    "                if metric_name == \"roc_auc_score\":\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                # Computing the evaluation metrics\n",
    "                results[model_name].append(metric_fn(y_test, y_pred))\n",
    "            end = time.time()\n",
    "\n",
    "            # Computing the runtime of model on the metric in ms\n",
    "            run_times[model_name] = (end - start) * 1000\n",
    "            # Computing the mean of the results for each metric\n",
    "            mean_results[model_name] = np.mean(results[model_name])\n",
    "            # Adding the data to the report table\n",
    "            table.add_data(\n",
    "                df_name, model_name, metric_name, mean_results[model_name], run_times[model_name]\n",
    "            )\n",
    "\n",
    "        # Creating the plot for the metric\n",
    "        colors = [\"red\", \"orange\", \"blue\", \"cyan\"]\n",
    "        axis[ax_ind].bar(list(range(len(models))), list(mean_results.values()), color=colors)\n",
    "        axis[ax_ind].set_ylim(\n",
    "            [min(mean_results.values()) - 0.1, min(max(mean_results.values()) + 0.1, 1)]\n",
    "        )\n",
    "        axis[ax_ind].set_xticks(list(range(len(models))))\n",
    "        axis[ax_ind].set_xticklabels(list(mean_results.keys()), rotation=30)\n",
    "        axis[ax_ind].set_title(f\"{metric_name} mean\", fontsize=18)\n",
    "        axis[ax_ind].set_ylabel(\"Mean Value\")\n",
    "        axis[ax_ind].tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "        axis[ax_ind].tick_params(axis=\"both\", which=\"minor\", labelsize=10)\n",
    "        ax_ind += 1\n",
    "        # Creating the plot for the runtime\n",
    "        if ax_ind == 5:\n",
    "            axis[ax_ind].bar(list(range(len(models))), list(run_times.values()), color=colors)\n",
    "            axis[ax_ind].set_ylim(\n",
    "                [min(min(run_times.values()) - 5, 0), max(run_times.values()) + 5]\n",
    "            )\n",
    "            axis[ax_ind].set_xticks(list(range(len(models))))\n",
    "            axis[ax_ind].set_xticklabels(list(run_times.keys()), rotation=30)\n",
    "            axis[ax_ind].set_title(\"Run times\", fontsize=18)\n",
    "            axis[ax_ind].set_ylabel(\"ms\")\n",
    "            axis[ax_ind].tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "            axis[ax_ind].tick_params(axis=\"both\", which=\"minor\", labelsize=10)\n",
    "    fig.suptitle(df_name, fontsize=30)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    # Logging the subplots for the dataset\n",
    "    wandb.log({df_name: wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Evaluates the models per dataset\n",
    "wandb.login(key=\"<API key here>\")\n",
    "report_table = wandb.Table(\n",
    "    columns=[\"Dataset\", \"Method\", \"Evaluation metric\", \"Evaluation Value\", \"Fit Runtime (in ms)\"]\n",
    ")\n",
    "\n",
    "for df_name, df in DFS.items():\n",
    "    X, y = prepare_data(df)\n",
    "    evaluate_models(X, y, METRICS, df_name, report_table)\n",
    "\n",
    "# Log the report table\n",
    "wandb.log({\"Report Table\": report_table})\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
